{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import config_loader\n",
    "from models.baseline import Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-28 01:11:52] [INFO]: {'with_test': True, 'log_iter': 10, 'save_iter': 1000, 'total_iter': 10000, 'restore_ckpt_strict': True, 'restore_hint': 0, 'optimizer_reset': False, 'scheduler_reset': False, 'save_path': 'output', 'save_name': 'Baseline', 'model_cfg': {'model': 'Baseline', 'backbone_cfg': {'block': 'BasicBlock', 'in_channel': 3, 'channels': [32, 64, 128, 256], 'layers': [1, 2, 2, 1], 'strides': [1, 2, 2, 1], 'maxpool': True}, 'SeparateFCs': {'in_channels': 256, 'out_channels': 128, 'parts_num': 31}, 'SeparateBNNecks': {'class_num': 43, 'in_channels': 128, 'parts_num': 31}, 'bin_num': [16, 8, 4, 2, 1]}, 'loss_cfg': {'triplet': {'loss_term_weight': 1.0, 'margin': 0.2}, 'softmax': {'loss_term_weight': 1.0, 'scale': 16, 'log_accuracy': True}}, 'optimizer_cfg': {'lr': 0.1, 'momentum': 0.9, 'weight_decay': 0.0005}, 'scheduler_cfg': {'gamma': 0.1, 'milestones': [20000, 40000, 50000]}, 'data_cfg': {'dataset_name': 'GTSRB', 'root_dir': 'datasets/GTSRB_cropped', 'train_path': 'datasets/GTSRB_cropped/Train.csv', 'test_path': 'datasets/GTSRB_cropped/Test.csv', 'meta_path': 'datasets/GTSRB_cropped/Meta.csv', 'num_workers': 1, 'RandomRotate': {'prob': 0.2, 'degree': 10}, 'RandomBrightness': {'prob': 0.2, 'delta': [-0.1, 0.1]}, 'RandomBlur': {'prob': 0.2, 'size': 5}, 'train_sampler': {'batch_shuffle': True, 'batch_size': [4, 8]}, 'test_sampler': {'batch_size': 32}}}\n",
      "[2025-03-28 01:11:54] [INFO]: Iteration 00010, Cost 4.06s, triplet_loss=1.6939, triplet_hard_loss=7.4953, triplet_loss_num=2272.8064, triplet_mean_dist=4.2253, softmax_loss=5.3953, softmax_accuracy=0.0386\n",
      "[2025-03-28 01:11:55] [INFO]: Iteration 00020, Cost 0.30s, triplet_loss=1.9161, triplet_hard_loss=8.1885, triplet_loss_num=2035.7549, triplet_mean_dist=4.7834, softmax_loss=5.3778, softmax_accuracy=0.0286\n",
      "[2025-03-28 01:11:55] [INFO]: Iteration 00030, Cost 0.28s, triplet_loss=1.3743, triplet_hard_loss=5.6180, triplet_loss_num=2104.8096, triplet_mean_dist=3.4709, softmax_loss=5.0479, softmax_accuracy=0.0364\n",
      "[2025-03-28 01:11:55] [INFO]: Iteration 00040, Cost 0.31s, triplet_loss=0.9541, triplet_hard_loss=3.9948, triplet_loss_num=2409.1387, triplet_mean_dist=2.2615, softmax_loss=4.9060, softmax_accuracy=0.0665\n",
      "[2025-03-28 01:11:56] [INFO]: Iteration 00050, Cost 0.26s, triplet_loss=0.6545, triplet_hard_loss=2.6720, triplet_loss_num=2453.1387, triplet_mean_dist=1.5740, softmax_loss=4.7003, softmax_accuracy=0.0491\n",
      "[2025-03-28 01:11:56] [INFO]: Iteration 00060, Cost 0.23s, triplet_loss=0.4390, triplet_hard_loss=1.7625, triplet_loss_num=2727.0225, triplet_mean_dist=1.0192, softmax_loss=4.4323, softmax_accuracy=0.0485\n",
      "[2025-03-28 01:11:56] [INFO]: Iteration 00070, Cost 0.25s, triplet_loss=0.3923, triplet_hard_loss=1.6575, triplet_loss_num=2884.4517, triplet_mean_dist=0.8068, softmax_loss=4.4013, softmax_accuracy=0.0707\n",
      "[2025-03-28 01:11:56] [INFO]: Iteration 00080, Cost 0.23s, triplet_loss=0.4113, triplet_hard_loss=1.6995, triplet_loss_num=2619.8162, triplet_mean_dist=0.9302, softmax_loss=4.3163, softmax_accuracy=0.0257\n",
      "[2025-03-28 01:11:57] [INFO]: Iteration 00090, Cost 0.26s, triplet_loss=0.4320, triplet_hard_loss=1.7178, triplet_loss_num=2534.9065, triplet_mean_dist=1.0067, softmax_loss=4.2327, softmax_accuracy=0.0598\n",
      "[2025-03-28 01:11:57] [INFO]: Iteration 00100, Cost 0.27s, triplet_loss=0.3947, triplet_hard_loss=1.5837, triplet_loss_num=2621.6548, triplet_mean_dist=0.8730, softmax_loss=4.2241, softmax_accuracy=0.0522\n",
      "[2025-03-28 01:11:57] [INFO]: Iteration 00110, Cost 0.23s, triplet_loss=0.3812, triplet_hard_loss=1.5145, triplet_loss_num=3106.7356, triplet_mean_dist=0.7786, softmax_loss=4.2927, softmax_accuracy=0.0367\n",
      "[2025-03-28 01:11:57] [INFO]: Iteration 00120, Cost 0.24s, triplet_loss=0.3439, triplet_hard_loss=1.3568, triplet_loss_num=2774.9580, triplet_mean_dist=0.7041, softmax_loss=4.0134, softmax_accuracy=0.0480\n",
      "[2025-03-28 01:11:58] [INFO]: Iteration 00130, Cost 0.24s, triplet_loss=0.3487, triplet_hard_loss=1.4438, triplet_loss_num=2730.8162, triplet_mean_dist=0.7687, softmax_loss=4.0191, softmax_accuracy=0.0651\n",
      "[2025-03-28 01:11:58] [INFO]: Iteration 00140, Cost 0.26s, triplet_loss=0.3693, triplet_hard_loss=1.5110, triplet_loss_num=2914.9419, triplet_mean_dist=0.7729, softmax_loss=3.8956, softmax_accuracy=0.0987\n",
      "[2025-03-28 01:11:58] [INFO]: Iteration 00150, Cost 0.26s, triplet_loss=0.3339, triplet_hard_loss=1.3082, triplet_loss_num=3180.9517, triplet_mean_dist=0.6470, softmax_loss=4.1302, softmax_accuracy=0.0500\n",
      "[2025-03-28 01:11:58] [INFO]: Iteration 00160, Cost 0.25s, triplet_loss=0.3191, triplet_hard_loss=1.2391, triplet_loss_num=3460.7839, triplet_mean_dist=0.6201, softmax_loss=4.0913, softmax_accuracy=0.0427\n",
      "[2025-03-28 01:11:59] [INFO]: Iteration 00170, Cost 0.26s, triplet_loss=0.3267, triplet_hard_loss=1.2923, triplet_loss_num=2726.1418, triplet_mean_dist=0.7122, softmax_loss=3.7729, softmax_accuracy=0.0841\n",
      "[2025-03-28 01:11:59] [INFO]: Iteration 00180, Cost 0.26s, triplet_loss=0.2966, triplet_hard_loss=1.1579, triplet_loss_num=2828.5710, triplet_mean_dist=0.6187, softmax_loss=3.8844, softmax_accuracy=0.0602\n",
      "[2025-03-28 01:11:59] [INFO]: Iteration 00190, Cost 0.28s, triplet_loss=0.3071, triplet_hard_loss=1.1994, triplet_loss_num=3082.4775, triplet_mean_dist=0.6279, softmax_loss=3.8569, softmax_accuracy=0.1000\n",
      "[2025-03-28 01:11:59] [INFO]: Iteration 00200, Cost 0.29s, triplet_loss=0.3384, triplet_hard_loss=1.4169, triplet_loss_num=2535.8420, triplet_mean_dist=0.7720, softmax_loss=3.8999, softmax_accuracy=0.0875\n",
      "[2025-03-28 01:12:00] [INFO]: Iteration 00210, Cost 0.25s, triplet_loss=0.3017, triplet_hard_loss=1.1901, triplet_loss_num=2507.1807, triplet_mean_dist=0.6948, softmax_loss=3.8375, softmax_accuracy=0.0766\n",
      "[2025-03-28 01:12:00] [INFO]: Iteration 00220, Cost 0.25s, triplet_loss=0.2991, triplet_hard_loss=1.2500, triplet_loss_num=2587.6484, triplet_mean_dist=0.6957, softmax_loss=3.7345, softmax_accuracy=0.1784\n",
      "[2025-03-28 01:12:00] [INFO]: Iteration 00230, Cost 0.25s, triplet_loss=0.2789, triplet_hard_loss=1.1414, triplet_loss_num=2952.6870, triplet_mean_dist=0.5835, softmax_loss=3.9894, softmax_accuracy=0.0811\n",
      "[2025-03-28 01:12:01] [INFO]: Iteration 00240, Cost 0.27s, triplet_loss=0.2763, triplet_hard_loss=1.1561, triplet_loss_num=2925.9646, triplet_mean_dist=0.5944, softmax_loss=3.8612, softmax_accuracy=0.1050\n",
      "[2025-03-28 01:12:01] [INFO]: Iteration 00250, Cost 0.25s, triplet_loss=0.3228, triplet_hard_loss=1.4588, triplet_loss_num=2488.4194, triplet_mean_dist=0.7138, softmax_loss=3.8058, softmax_accuracy=0.0739\n",
      "[2025-03-28 01:12:01] [INFO]: Iteration 00260, Cost 0.25s, triplet_loss=0.3280, triplet_hard_loss=1.3438, triplet_loss_num=2343.4709, triplet_mean_dist=0.8257, softmax_loss=3.8478, softmax_accuracy=0.1079\n",
      "[2025-03-28 01:12:01] [INFO]: Iteration 00270, Cost 0.27s, triplet_loss=0.3541, triplet_hard_loss=1.4253, triplet_loss_num=2786.0193, triplet_mean_dist=0.7966, softmax_loss=3.9176, softmax_accuracy=0.0618\n",
      "[2025-03-28 01:12:02] [INFO]: Iteration 00280, Cost 0.27s, triplet_loss=0.2954, triplet_hard_loss=1.1768, triplet_loss_num=3009.7935, triplet_mean_dist=0.6105, softmax_loss=3.9338, softmax_accuracy=0.0775\n",
      "[2025-03-28 01:12:02] [INFO]: Iteration 00290, Cost 0.24s, triplet_loss=0.3078, triplet_hard_loss=1.2813, triplet_loss_num=2735.0840, triplet_mean_dist=0.6550, softmax_loss=3.8419, softmax_accuracy=0.0555\n",
      "[2025-03-28 01:12:02] [INFO]: Iteration 00300, Cost 0.25s, triplet_loss=0.3218, triplet_hard_loss=1.3937, triplet_loss_num=2938.0967, triplet_mean_dist=0.6811, softmax_loss=3.8242, softmax_accuracy=0.0727\n",
      "[2025-03-28 01:12:02] [INFO]: Iteration 00310, Cost 0.23s, triplet_loss=0.2988, triplet_hard_loss=1.2660, triplet_loss_num=2685.4807, triplet_mean_dist=0.6498, softmax_loss=3.8430, softmax_accuracy=0.0596\n",
      "[2025-03-28 01:12:03] [INFO]: Iteration 00320, Cost 0.24s, triplet_loss=0.2967, triplet_hard_loss=1.2121, triplet_loss_num=2987.2227, triplet_mean_dist=0.6244, softmax_loss=3.7206, softmax_accuracy=0.1238\n",
      "[2025-03-28 01:12:03] [INFO]: Iteration 00330, Cost 0.24s, triplet_loss=0.2727, triplet_hard_loss=1.1356, triplet_loss_num=3132.2483, triplet_mean_dist=0.5616, softmax_loss=3.7384, softmax_accuracy=0.0833\n",
      "[2025-03-28 01:12:03] [INFO]: Iteration 00340, Cost 0.28s, triplet_loss=0.2591, triplet_hard_loss=1.1164, triplet_loss_num=2727.3452, triplet_mean_dist=0.5650, softmax_loss=3.5278, softmax_accuracy=0.1516\n",
      "[2025-03-28 01:12:03] [INFO]: Iteration 00350, Cost 0.24s, triplet_loss=0.2753, triplet_hard_loss=1.1199, triplet_loss_num=2932.2419, triplet_mean_dist=0.5900, softmax_loss=3.7645, softmax_accuracy=0.0550\n",
      "[2025-03-28 01:12:04] [INFO]: Iteration 00360, Cost 0.28s, triplet_loss=0.2651, triplet_hard_loss=1.0985, triplet_loss_num=2855.4548, triplet_mean_dist=0.5552, softmax_loss=3.6624, softmax_accuracy=0.1175\n",
      "[2025-03-28 01:12:04] [INFO]: Iteration 00370, Cost 0.25s, triplet_loss=0.2421, triplet_hard_loss=0.9463, triplet_loss_num=2685.1646, triplet_mean_dist=0.5558, softmax_loss=3.6780, softmax_accuracy=0.0966\n",
      "[2025-03-28 01:12:04] [INFO]: Iteration 00380, Cost 0.23s, triplet_loss=0.2695, triplet_hard_loss=1.1041, triplet_loss_num=2467.7549, triplet_mean_dist=0.6431, softmax_loss=3.6541, softmax_accuracy=0.0798\n",
      "[2025-03-28 01:12:04] [INFO]: Iteration 00390, Cost 0.24s, triplet_loss=0.2919, triplet_hard_loss=1.2167, triplet_loss_num=2825.8806, triplet_mean_dist=0.6152, softmax_loss=3.5910, softmax_accuracy=0.1468\n",
      "[2025-03-28 01:12:05] [INFO]: Iteration 00400, Cost 0.30s, triplet_loss=0.2603, triplet_hard_loss=1.0913, triplet_loss_num=2543.1968, triplet_mean_dist=0.5732, softmax_loss=3.6860, softmax_accuracy=0.0724\n",
      "[2025-03-28 01:12:05] [INFO]: Iteration 00410, Cost 0.29s, triplet_loss=0.2761, triplet_hard_loss=1.1736, triplet_loss_num=2560.4226, triplet_mean_dist=0.6081, softmax_loss=3.5548, softmax_accuracy=0.1390\n",
      "[2025-03-28 01:12:05] [INFO]: Iteration 00420, Cost 0.28s, triplet_loss=0.2777, triplet_hard_loss=1.2597, triplet_loss_num=2381.3936, triplet_mean_dist=0.6475, softmax_loss=3.7445, softmax_accuracy=0.0952\n",
      "[2025-03-28 01:12:05] [INFO]: Iteration 00430, Cost 0.29s, triplet_loss=0.2560, triplet_hard_loss=1.0788, triplet_loss_num=2605.5549, triplet_mean_dist=0.5931, softmax_loss=3.6150, softmax_accuracy=0.1033\n",
      "[2025-03-28 01:12:06] [INFO]: Iteration 00440, Cost 0.27s, triplet_loss=0.2952, triplet_hard_loss=1.5493, triplet_loss_num=2370.8872, triplet_mean_dist=0.6693, softmax_loss=3.6375, softmax_accuracy=0.0861\n",
      "[2025-03-28 01:12:06] [INFO]: Iteration 00450, Cost 0.27s, triplet_loss=0.3474, triplet_hard_loss=1.6358, triplet_loss_num=2384.7227, triplet_mean_dist=0.7782, softmax_loss=3.6375, softmax_accuracy=0.0992\n",
      "[2025-03-28 01:12:06] [INFO]: Iteration 00460, Cost 0.29s, triplet_loss=0.2832, triplet_hard_loss=1.2925, triplet_loss_num=2415.5613, triplet_mean_dist=0.6751, softmax_loss=3.5220, softmax_accuracy=0.1184\n",
      "[2025-03-28 01:12:07] [INFO]: Iteration 00470, Cost 0.25s, triplet_loss=0.2406, triplet_hard_loss=1.0131, triplet_loss_num=2025.3904, triplet_mean_dist=0.6947, softmax_loss=3.5344, softmax_accuracy=0.1275\n",
      "[2025-03-28 01:12:07] [INFO]: Iteration 00480, Cost 0.26s, triplet_loss=0.2773, triplet_hard_loss=1.1755, triplet_loss_num=2415.0710, triplet_mean_dist=0.6927, softmax_loss=3.6186, softmax_accuracy=0.1093\n",
      "[2025-03-28 01:12:07] [INFO]: Iteration 00490, Cost 0.28s, triplet_loss=0.2680, triplet_hard_loss=1.1912, triplet_loss_num=1961.0258, triplet_mean_dist=0.7719, softmax_loss=3.2853, softmax_accuracy=0.1511\n",
      "[2025-03-28 01:12:07] [INFO]: Iteration 00500, Cost 0.28s, triplet_loss=0.2744, triplet_hard_loss=1.1699, triplet_loss_num=2285.4580, triplet_mean_dist=0.7569, softmax_loss=3.6862, softmax_accuracy=0.1259\n",
      "[2025-03-28 01:12:08] [INFO]: Iteration 00510, Cost 0.25s, triplet_loss=0.2481, triplet_hard_loss=1.0501, triplet_loss_num=1885.4935, triplet_mean_dist=0.7487, softmax_loss=3.5095, softmax_accuracy=0.1195\n",
      "[2025-03-28 01:12:08] [INFO]: Iteration 00520, Cost 0.27s, triplet_loss=0.2751, triplet_hard_loss=1.1353, triplet_loss_num=2094.5676, triplet_mean_dist=0.8020, softmax_loss=3.4464, softmax_accuracy=0.1428\n",
      "[2025-03-28 01:12:08] [INFO]: Iteration 00530, Cost 0.24s, triplet_loss=0.2405, triplet_hard_loss=1.0194, triplet_loss_num=2499.7065, triplet_mean_dist=0.6470, softmax_loss=3.2689, softmax_accuracy=0.1803\n",
      "[2025-03-28 01:12:08] [INFO]: Iteration 00540, Cost 0.27s, triplet_loss=0.2298, triplet_hard_loss=0.9623, triplet_loss_num=2020.8710, triplet_mean_dist=0.7049, softmax_loss=3.5111, softmax_accuracy=0.1235\n",
      "[2025-03-28 01:12:09] [INFO]: Iteration 00550, Cost 0.24s, triplet_loss=0.2688, triplet_hard_loss=1.1280, triplet_loss_num=1810.7581, triplet_mean_dist=0.8182, softmax_loss=3.2682, softmax_accuracy=0.1907\n",
      "[2025-03-28 01:12:09] [INFO]: Iteration 00560, Cost 0.23s, triplet_loss=0.2797, triplet_hard_loss=1.1924, triplet_loss_num=1699.6677, triplet_mean_dist=0.8896, softmax_loss=3.3575, softmax_accuracy=0.1536\n",
      "[2025-03-28 01:12:09] [INFO]: Iteration 00570, Cost 0.27s, triplet_loss=0.2931, triplet_hard_loss=1.2618, triplet_loss_num=1667.7225, triplet_mean_dist=0.9194, softmax_loss=3.4841, softmax_accuracy=0.1149\n",
      "[2025-03-28 01:12:09] [INFO]: Iteration 00580, Cost 0.27s, triplet_loss=0.2579, triplet_hard_loss=1.0866, triplet_loss_num=2170.1453, triplet_mean_dist=0.7487, softmax_loss=3.7393, softmax_accuracy=0.1024\n",
      "[2025-03-28 01:12:10] [INFO]: Iteration 00590, Cost 0.27s, triplet_loss=0.2816, triplet_hard_loss=1.1793, triplet_loss_num=2237.0032, triplet_mean_dist=0.7501, softmax_loss=3.6602, softmax_accuracy=0.0814\n",
      "[2025-03-28 01:12:10] [INFO]: Iteration 00600, Cost 0.29s, triplet_loss=0.2673, triplet_hard_loss=1.1349, triplet_loss_num=1831.6677, triplet_mean_dist=0.8070, softmax_loss=3.3262, softmax_accuracy=0.1414\n",
      "[2025-03-28 01:12:10] [INFO]: Iteration 00610, Cost 0.30s, triplet_loss=0.2523, triplet_hard_loss=1.0855, triplet_loss_num=1516.2064, triplet_mean_dist=0.8633, softmax_loss=3.4186, softmax_accuracy=0.1546\n",
      "[2025-03-28 01:12:11] [INFO]: Iteration 00620, Cost 0.29s, triplet_loss=0.2437, triplet_hard_loss=0.9913, triplet_loss_num=1705.8871, triplet_mean_dist=0.8438, softmax_loss=3.3768, softmax_accuracy=0.1231\n",
      "[2025-03-28 01:12:11] [INFO]: Iteration 00630, Cost 0.26s, triplet_loss=0.2537, triplet_hard_loss=1.0655, triplet_loss_num=1844.0065, triplet_mean_dist=0.8331, softmax_loss=3.3848, softmax_accuracy=0.1608\n",
      "[2025-03-28 01:12:11] [INFO]: Iteration 00640, Cost 0.29s, triplet_loss=0.2318, triplet_hard_loss=1.0353, triplet_loss_num=1288.2484, triplet_mean_dist=0.9175, softmax_loss=3.2384, softmax_accuracy=0.1775\n",
      "[2025-03-28 01:12:11] [INFO]: Iteration 00650, Cost 0.24s, triplet_loss=0.2642, triplet_hard_loss=1.1264, triplet_loss_num=1807.2323, triplet_mean_dist=0.8800, softmax_loss=3.5745, softmax_accuracy=0.1154\n",
      "[2025-03-28 01:12:12] [INFO]: Iteration 00660, Cost 0.32s, triplet_loss=0.2398, triplet_hard_loss=0.9868, triplet_loss_num=1337.3806, triplet_mean_dist=0.9011, softmax_loss=3.1857, softmax_accuracy=0.1718\n",
      "[2025-03-28 01:12:12] [INFO]: Iteration 00670, Cost 0.27s, triplet_loss=0.2450, triplet_hard_loss=0.9736, triplet_loss_num=1541.8484, triplet_mean_dist=0.8434, softmax_loss=3.1715, softmax_accuracy=0.2070\n",
      "[2025-03-28 01:12:12] [INFO]: Iteration 00680, Cost 0.26s, triplet_loss=0.2868, triplet_hard_loss=1.2081, triplet_loss_num=1758.4354, triplet_mean_dist=0.9068, softmax_loss=3.4514, softmax_accuracy=0.1398\n",
      "[2025-03-28 01:12:12] [INFO]: Iteration 00690, Cost 0.26s, triplet_loss=0.2632, triplet_hard_loss=1.1483, triplet_loss_num=1512.1194, triplet_mean_dist=0.9486, softmax_loss=3.0986, softmax_accuracy=0.2356\n",
      "[2025-03-28 01:12:13] [INFO]: Iteration 00700, Cost 0.26s, triplet_loss=0.2664, triplet_hard_loss=1.1152, triplet_loss_num=1614.7903, triplet_mean_dist=0.9627, softmax_loss=3.2930, softmax_accuracy=0.1760\n",
      "[2025-03-28 01:12:13] [INFO]: Iteration 00710, Cost 0.22s, triplet_loss=0.2408, triplet_hard_loss=1.0427, triplet_loss_num=1551.7775, triplet_mean_dist=0.8558, softmax_loss=3.2518, softmax_accuracy=0.1634\n",
      "[2025-03-28 01:12:13] [INFO]: Iteration 00720, Cost 0.25s, triplet_loss=0.2662, triplet_hard_loss=1.1186, triplet_loss_num=1677.4452, triplet_mean_dist=0.9317, softmax_loss=3.2615, softmax_accuracy=0.1643\n",
      "[2025-03-28 01:12:13] [INFO]: Iteration 00730, Cost 0.24s, triplet_loss=0.2575, triplet_hard_loss=1.0670, triplet_loss_num=1594.4452, triplet_mean_dist=0.9107, softmax_loss=3.3299, softmax_accuracy=0.1720\n",
      "[2025-03-28 01:12:14] [INFO]: Iteration 00740, Cost 0.22s, triplet_loss=0.2753, triplet_hard_loss=1.1462, triplet_loss_num=1907.2032, triplet_mean_dist=0.8704, softmax_loss=3.2001, softmax_accuracy=0.1958\n",
      "[2025-03-28 01:12:14] [INFO]: Iteration 00750, Cost 0.25s, triplet_loss=0.2137, triplet_hard_loss=0.8925, triplet_loss_num=1343.3967, triplet_mean_dist=0.8020, softmax_loss=3.0431, softmax_accuracy=0.2264\n",
      "[2025-03-28 01:12:14] [INFO]: Iteration 00760, Cost 0.28s, triplet_loss=0.2537, triplet_hard_loss=1.0878, triplet_loss_num=1680.4130, triplet_mean_dist=0.8463, softmax_loss=3.3194, softmax_accuracy=0.1964\n",
      "[2025-03-28 01:12:14] [INFO]: Iteration 00770, Cost 0.25s, triplet_loss=0.2241, triplet_hard_loss=0.9843, triplet_loss_num=1374.4742, triplet_mean_dist=0.9148, softmax_loss=3.0068, softmax_accuracy=0.2098\n",
      "[2025-03-28 01:12:15] [INFO]: Iteration 00780, Cost 0.23s, triplet_loss=0.2320, triplet_hard_loss=0.9671, triplet_loss_num=1369.2484, triplet_mean_dist=0.8736, softmax_loss=2.9641, softmax_accuracy=0.2302\n",
      "[2025-03-28 01:12:15] [INFO]: Iteration 00790, Cost 0.28s, triplet_loss=0.2344, triplet_hard_loss=1.0094, triplet_loss_num=1551.4871, triplet_mean_dist=0.9048, softmax_loss=3.0989, softmax_accuracy=0.2204\n",
      "[2025-03-28 01:12:15] [INFO]: Iteration 00800, Cost 0.24s, triplet_loss=0.2435, triplet_hard_loss=1.0155, triplet_loss_num=1054.6096, triplet_mean_dist=1.0680, softmax_loss=2.9012, softmax_accuracy=0.2619\n",
      "[2025-03-28 01:12:15] [INFO]: Iteration 00810, Cost 0.25s, triplet_loss=0.2420, triplet_hard_loss=1.0011, triplet_loss_num=1017.6516, triplet_mean_dist=1.1526, softmax_loss=2.8505, softmax_accuracy=0.2431\n",
      "[2025-03-28 01:12:16] [INFO]: Iteration 00820, Cost 0.24s, triplet_loss=0.2652, triplet_hard_loss=1.1267, triplet_loss_num=1728.7936, triplet_mean_dist=0.9661, softmax_loss=3.1493, softmax_accuracy=0.2364\n",
      "[2025-03-28 01:12:16] [INFO]: Iteration 00830, Cost 0.21s, triplet_loss=0.2389, triplet_hard_loss=0.9849, triplet_loss_num=1261.0968, triplet_mean_dist=1.0502, softmax_loss=2.9153, softmax_accuracy=0.2935\n",
      "[2025-03-28 01:12:16] [INFO]: Iteration 00840, Cost 0.22s, triplet_loss=0.2136, triplet_hard_loss=0.8845, triplet_loss_num=903.1710, triplet_mean_dist=1.0376, softmax_loss=2.7677, softmax_accuracy=0.2851\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m cfgs \u001b[38;5;241m=\u001b[39m config_loader(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs/default.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Baseline(cfgs)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mBaseline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Traffic-Sign-Recognition/models/baseline.py:257\u001b[0m, in \u001b[0;36mBaseline.run_train\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m retval\n\u001b[1;32m    256\u001b[0m loss_sum, loss_info \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss_aggregator(training_feat)\n\u001b[0;32m--> 257\u001b[0m ok \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Traffic-Sign-Recognition/models/baseline.py:208\u001b[0m, in \u001b[0;36mBaseline.train_step\u001b[0;34m(self, loss_sum)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsg_mgr\u001b[38;5;241m.\u001b[39mlog_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFind the loss sum less than 1e-9 but the training process will continue!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[43mloss_sum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfgs = config_loader(path='configs/default.yaml')\n",
    "model = Baseline(cfgs)\n",
    "Baseline.run_train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Baseline(model, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
